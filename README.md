# WSB Stock Analyzer ğŸš€

Ein umfassendes Tool zur Analyse von AktienerwÃ¤hnungen im Subreddit r/wallstreetbets. Dieses Projekt bietet eine Web-OberflÃ¤che mit Streamlit, um die neuesten Trends zu crawlen, zu analysieren und zu visualisieren.

![Streamlit App Screenshot](https://user-images.githubusercontent.com/12345/placeholder.png) _(Hinweis: FÃ¼gen Sie hier einen Screenshot der laufenden Anwendung ein)_

## âœ¨ Features

- **Reddit Crawler**: Sammelt Posts und Kommentare von r/wallstreetbets und extrahiert ErwÃ¤hnungen von Aktiensymbolen.
- **Datenanalyse**: Verarbeitet die gesammelten Daten, um Trends, Top-ErwÃ¤hnungen und historische Daten zu analysieren.
- **Interaktive Web-OberflÃ¤che**: Eine mit Streamlit erstellte, benutzerfreundliche OberflÃ¤che zur Steuerung des Crawlers und zur Anzeige der Ergebnisse.
- **Visualisierungen**: Erstellt Diagramme und Heatmaps, um die Daten verstÃ¤ndlich darzustellen.
- **Lokale Speicherung**: Speichert Ergebnisse und Analysen lokal fÃ¼r zukÃ¼nftige Vergleiche.
- **Sichere Konfiguration**: Speichert Reddit API-Anmeldeinformationen sicher im lokalen Speicher des Browsers.
- **Alternative Desktop-GUI**: EnthÃ¤lt auch eine mit Tkinter erstellte Desktop-Anwendung.

## ğŸ—ï¸ Architektur

Das Projekt ist modular aufgebaut und besteht aus mehreren Kernkomponenten:

- `reddit_crawler.py`: Verantwortlich fÃ¼r die Verbindung zur Reddit-API und das Sammeln von Daten.
- `data_analyzer.py`: LÃ¤dt die gesammelten Daten, fÃ¼hrt Analysen durch und erstellt Visualisierungen.
- `streamlit_app.py`: Die empfohlene, moderne Web-Anwendung zur Interaktion mit dem Tool.
- `gui_app.py`: Eine alternative Desktop-Anwendung, die mit Tkinter erstellt wurde.
- `config.py`: Zentrale Konfigurationsdatei fÃ¼r Crawler-Einstellungen und Dateipfade.

## ğŸ“‹ Voraussetzungen

- Python 3.8+
- Ein Reddit-Konto und API-Anmeldeinformationen.

## ğŸ› ï¸ Installation & Konfiguration

Folgen Sie diesen Schritten, um das Projekt lokal einzurichten:

**1. Repository klonen**
```bash
git clone https://github.com/spiral023/wsb-analyzer.git
cd wsb-analyzer
```

**2. AbhÃ¤ngigkeiten installieren**
Es wird empfohlen, eine virtuelle Umgebung zu verwenden.
```bash
python -m venv venv
source venv/bin/activate  # Auf Windows: venv\Scripts\activate
pip install -r requirements.txt
```

**3. Reddit API Konfiguration**
Sie benÃ¶tigen API-Anmeldeinformationen von Reddit, um auf deren Daten zugreifen zu kÃ¶nnen.

- Gehen Sie zu [Reddit Apps](https://www.reddit.com/prefs/apps) und erstellen Sie eine neue App.
- WÃ¤hlen Sie den Typ `script`.
- Geben Sie einen Namen und eine `redirect uri` an (z.B. `http://localhost:8501`).
- Kopieren Sie die `client_id` (unter dem App-Namen) und den `client_secret`.

**FÃ¼r die Streamlit App (empfohlen):**
Die Anmeldeinformationen werden direkt in der Web-OberflÃ¤che eingegeben und sicher im lokalen Speicher Ihres Browsers gespeichert. Sie mÃ¼ssen keine `.env`-Datei manuell erstellen.

**FÃ¼r die Tkinter Desktop App:**
- Erstellen Sie eine Kopie der `.env.example`-Datei und benennen Sie sie in `.env` um.
```bash
cp .env.example .env
```
- Ã–ffnen Sie die `.env`-Datei und tragen Sie Ihre Reddit-Anmeldeinformationen ein:
```env
REDDIT_CLIENT_ID="Ihre_Client_ID"
REDDIT_CLIENT_SECRET="Ihr_Client_Secret"
REDDIT_USERNAME="Ihr_Reddit_Benutzername"
REDDIT_PASSWORD="Ihr_Reddit_Passwort"
```

## ğŸš€ Verwendung

### Streamlit Web App (Empfohlen)

Dies ist der einfachste Weg, die Anwendung zu nutzen.

1.  **Starten Sie die Streamlit-App:**
    ```bash
    streamlit run streamlit_app.py
    ```
2.  Ã–ffnen Sie Ihren Webbrowser und navigieren Sie zu der angezeigten lokalen URL (normalerweise `http://localhost:8501`).
3.  **Konfigurieren Sie die API-Daten:**
    - Geben Sie Ihre Reddit-API-Anmeldeinformationen in der Seitenleiste ein.
    - Klicken Sie auf "Speichern & Testen", um die Verbindung zu Ã¼berprÃ¼fen. Die Daten werden sicher im lokalen Speicher Ihres Browsers gespeichert.
4.  **Starten Sie das Crawling:**
    - Passen Sie bei Bedarf die Crawler-Einstellungen in der Seitenleiste an.
    - Klicken Sie im Dashboard auf "Crawling starten".
5.  **Starten Sie die Analyse:**
    - Sobald das Crawling abgeschlossen ist, klicken Sie auf "Analyse starten", um die Daten zu verarbeiten und Visualisierungen zu erstellen.

### Tkinter Desktop App

1.  Stellen Sie sicher, dass Sie die `.env`-Datei wie oben beschrieben konfiguriert haben.
2.  **Starten Sie die GUI-Anwendung:**
    ```bash
    python run_app.py
    ```
    oder
    ```bash
    python gui_app.py
    ```

## ğŸ“‚ Dateistruktur

```
.
â”œâ”€â”€ .env.example          # Vorlage fÃ¼r die Konfigurationsdatei
â”œâ”€â”€ .gitignore            # Git-Ignore-Datei
â”œâ”€â”€ config.py             # Zentrale Konfiguration
â”œâ”€â”€ data_analyzer.py      # Modul fÃ¼r die Datenanalyse
â”œâ”€â”€ gui_app.py            # Tkinter Desktop-Anwendung
â”œâ”€â”€ README.md             # Diese Datei
â”œâ”€â”€ reddit_crawler.py     # Modul zum Crawlen von Reddit
â”œâ”€â”€ requirements.txt      # Python-AbhÃ¤ngigkeiten
â”œâ”€â”€ run_app.py            # Startskript fÃ¼r die Tkinter-App
â”œâ”€â”€ streamlit_app.py      # Streamlit Web-Anwendung
â”œâ”€â”€ data/                 # Verzeichnis fÃ¼r Daten
â”‚   â”œâ”€â”€ stock_symbols.csv # Liste der Aktiensymbole
â”‚   â”œâ”€â”€ analysis/         # Gespeicherte Analyseergebnisse
â”‚   â””â”€â”€ results/          # Gespeicherte Crawling-Ergebnisse
â””â”€â”€ logs/                 # Log-Dateien
```

## ğŸ“„ Lizenz

Dieses Projekt steht unter der [MIT-Lizenz](LICENSE).
